{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "26daeb4f-6086-7c92-806b-56a5950c3564",
    "_uuid": "a804b2dfa955691fb42faf7117953aa411dc8f3e"
   },
   "source": [
    "An introduction for using scikit learn pipeline and plot metrics for creating the best model which is tuned hyper parameters. \n",
    "\n",
    "Topic\n",
    "\n",
    "* Cross Validation\n",
    "* Learning Curve\n",
    "* Validation Curve\n",
    "* Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "de546b87-6689-0865-fd50-5363359e3fcf",
    "_uuid": "92e281d651a284ad56781d08a78634c788157a33"
   },
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['ls', 'reconhecimento de genero por voz/voice.csv']' returned non-zero exit status 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-953ca1c14342>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msubprocess\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ls\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"reconhecimento de genero por voz/voice.csv\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Any results you write to the current directory are saved as output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0;32m--> 626\u001b[0;31m                **kwargs).stdout\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m             raise CalledProcessError(retcode, process.args,\n\u001b[0;32m--> 708\u001b[0;31m                                      output=stdout, stderr=stderr)\n\u001b[0m\u001b[1;32m    709\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mCompletedProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['ls', 'reconhecimento de genero por voz/voice.csv']' returned non-zero exit status 2"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"reconhecimento de genero por voz/voice.csv\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "d979b18f-26bd-b1a0-7051-bdafb229c3cd",
    "_uuid": "92b4b9a2c7461b7316ef994225cd4f08b5fb32c9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q75</th>\n",
       "      <th>IQR</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurt</th>\n",
       "      <th>sp.ent</th>\n",
       "      <th>sfm</th>\n",
       "      <th>...</th>\n",
       "      <th>centroid</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>meandom</th>\n",
       "      <th>mindom</th>\n",
       "      <th>maxdom</th>\n",
       "      <th>dfrange</th>\n",
       "      <th>modindx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.059781</td>\n",
       "      <td>0.064241</td>\n",
       "      <td>0.032027</td>\n",
       "      <td>0.015071</td>\n",
       "      <td>0.090193</td>\n",
       "      <td>0.075122</td>\n",
       "      <td>12.863462</td>\n",
       "      <td>274.402906</td>\n",
       "      <td>0.893369</td>\n",
       "      <td>0.491918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059781</td>\n",
       "      <td>0.084279</td>\n",
       "      <td>0.015702</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.066009</td>\n",
       "      <td>0.067310</td>\n",
       "      <td>0.040229</td>\n",
       "      <td>0.019414</td>\n",
       "      <td>0.092666</td>\n",
       "      <td>0.073252</td>\n",
       "      <td>22.423285</td>\n",
       "      <td>634.613855</td>\n",
       "      <td>0.892193</td>\n",
       "      <td>0.513724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066009</td>\n",
       "      <td>0.107937</td>\n",
       "      <td>0.015826</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.009014</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.077316</td>\n",
       "      <td>0.083829</td>\n",
       "      <td>0.036718</td>\n",
       "      <td>0.008701</td>\n",
       "      <td>0.131908</td>\n",
       "      <td>0.123207</td>\n",
       "      <td>30.757155</td>\n",
       "      <td>1024.927705</td>\n",
       "      <td>0.846389</td>\n",
       "      <td>0.478905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077316</td>\n",
       "      <td>0.098706</td>\n",
       "      <td>0.015656</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.007990</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.072111</td>\n",
       "      <td>0.158011</td>\n",
       "      <td>0.096582</td>\n",
       "      <td>0.207955</td>\n",
       "      <td>0.111374</td>\n",
       "      <td>1.232831</td>\n",
       "      <td>4.177296</td>\n",
       "      <td>0.963322</td>\n",
       "      <td>0.727232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.017798</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.201497</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>0.247119</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.135120</td>\n",
       "      <td>0.079146</td>\n",
       "      <td>0.124656</td>\n",
       "      <td>0.078720</td>\n",
       "      <td>0.206045</td>\n",
       "      <td>0.127325</td>\n",
       "      <td>1.101174</td>\n",
       "      <td>4.333713</td>\n",
       "      <td>0.971955</td>\n",
       "      <td>0.783568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135120</td>\n",
       "      <td>0.106398</td>\n",
       "      <td>0.016931</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.712812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>5.484375</td>\n",
       "      <td>5.476562</td>\n",
       "      <td>0.208274</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   meanfreq        sd    median       Q25       Q75       IQR       skew  \\\n",
       "0  0.059781  0.064241  0.032027  0.015071  0.090193  0.075122  12.863462   \n",
       "1  0.066009  0.067310  0.040229  0.019414  0.092666  0.073252  22.423285   \n",
       "2  0.077316  0.083829  0.036718  0.008701  0.131908  0.123207  30.757155   \n",
       "3  0.151228  0.072111  0.158011  0.096582  0.207955  0.111374   1.232831   \n",
       "4  0.135120  0.079146  0.124656  0.078720  0.206045  0.127325   1.101174   \n",
       "\n",
       "          kurt    sp.ent       sfm  ...    centroid   meanfun    minfun  \\\n",
       "0   274.402906  0.893369  0.491918  ...    0.059781  0.084279  0.015702   \n",
       "1   634.613855  0.892193  0.513724  ...    0.066009  0.107937  0.015826   \n",
       "2  1024.927705  0.846389  0.478905  ...    0.077316  0.098706  0.015656   \n",
       "3     4.177296  0.963322  0.727232  ...    0.151228  0.088965  0.017798   \n",
       "4     4.333713  0.971955  0.783568  ...    0.135120  0.106398  0.016931   \n",
       "\n",
       "     maxfun   meandom    mindom    maxdom   dfrange   modindx  label  \n",
       "0  0.275862  0.007812  0.007812  0.007812  0.000000  0.000000   male  \n",
       "1  0.250000  0.009014  0.007812  0.054688  0.046875  0.052632   male  \n",
       "2  0.271186  0.007990  0.007812  0.015625  0.007812  0.046512   male  \n",
       "3  0.250000  0.201497  0.007812  0.562500  0.554688  0.247119   male  \n",
       "4  0.266667  0.712812  0.007812  5.484375  5.476562  0.208274   male  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./voice.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d113c589-5873-8a44-735c-0e578b89b0c1",
    "_uuid": "bbbc03bd947705990c40b4f73ef864ce0ba06012",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Total number of samples: {}\".format(df.shape[0]))\n",
    "print(\"Number of male: {}\".format(df[df.label == 'male'].shape[0]))\n",
    "print(\"Number of female: {}\".format(df[df.label == 'female'].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d0335123-5a34-95a7-30d2-1b8b3af1a488",
    "_uuid": "f51947df9f4f5cdc2c598875c611d08433e5e548"
   },
   "source": [
    "The target label of this classification is in \"label\" column and others are features.\n",
    "These two categories (\"male\", \"female\") are just half and half. It seems no skew in this data set.\n",
    "First we confirm to there is any missing values in this data sets. This is a common case a dataset in real world has missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "64c7507c-d332-71e8-9de7-be7d8dd72b35",
    "_uuid": "69494d64836b736eb0e134c530e06422e2ab781c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f4f8d7d1-ca5b-f9c8-8b26-27e9ffed3ca9",
    "_uuid": "22d0c1e7e03c015f41c98a966f074977acef6ceb"
   },
   "source": [
    "Okay we confirm there is not any missing values in any columns. Before creating learning pipeline it is not a bad idea to visualize overview and the relationship with each features. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "51f4685e-3c45-cc8a-1e5f-ce226331fda7",
    "_uuid": "37f47a5d3021678a9f70cbe4b2e10a70b45aa874",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import seaborn\n",
    "df.head()\n",
    "df.plot(kind='scatter', x='meanfreq', y='dfrange')\n",
    "df.plot(kind='kde', y='meanfreq')\n",
    "#seaborn.pairplot(df['meanfreq', 'sd', 'skew'], hue='label', size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2185b150-a0db-5a42-a03d-f9a7651e957b",
    "_uuid": "ea604d86afc38d7663c74fccc98981f418438968"
   },
   "source": [
    "You can also do this easily with seaborn for visializing multiple feature relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3ad204a3-cf33-a9e2-42dd-126797084970",
    "_uuid": "ac52a5838460561fdcd07b8cfff5473e4c661a13",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seaborn.pairplot(df[['meanfreq', 'Q25', 'Q75', 'skew', 'centroid', 'label']], \n",
    "                 hue='label', size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "25e1e66d-c324-6e89-30a7-4c6a4b2952fe",
    "_uuid": "59edca86256640d1e61a41ef9cffddd9ffb99e2b"
   },
   "source": [
    "These information sometimes can be useful to select features to be used for training model.\n",
    "So now is the time to create training logic. Data set first should be separated with training data and test data for evaluating trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "786b776d-6304-4cd9-ef92-c2ab5545c056",
    "_uuid": "065391c33c86b0e6d455a2810d3ebc422eb5ca56",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X, y = df.iloc[:, :-1].values, df.iloc[:, -1].values\n",
    "\n",
    "# Encode label category\n",
    "# male -> 1\n",
    "# female -> 0\n",
    "\n",
    "gender_encoder = LabelEncoder()\n",
    "y = gender_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ea3395f7-f86c-9a89-b411-d07fc7176c0b",
    "_uuid": "5655ed8f62818ba860e034882c9ccdfc33b3a122"
   },
   "source": [
    "Since scikit-learn provide Pipeline API which enables us to create preprocessing and machine learning model at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ba9394bf-8f8a-1bfe-e124-1b6a38caa821",
    "_uuid": "30b1fcbda8c8dd619e3836f0ec53ae9cbf67e7bf",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe_svc = Pipeline([('std_scl', StandardScaler()), \n",
    "                    ('pca', PCA(n_components=10)),\n",
    "                    ('svc', SVC(random_state=1))])\n",
    "\n",
    "pipe_svc.fit(X_train, y_train)\n",
    "\n",
    "print('Test Accuracy: %.3f' % pipe_svc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d1aa9cd6-010d-8c95-ac6b-2f62b52160f6",
    "_uuid": "d6009c0d13032536bc75352f9871e92b7d277160"
   },
   "source": [
    "At first grance we already gained pretty good accuracy which is over 97%.\n",
    "But we might have a room for improvements. Now we evaluate this model from various type of metrics.\n",
    "\n",
    "# Cross Validation\n",
    "\n",
    "cross validation can provide more general metric of this model and it can enables us to reduce variance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c852856a-59e0-860b-b1eb-c37a5580ce83",
    "_uuid": "fa8b9942785f772bb4cf1385c8344c3e7e0ca302",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(estimator=pipe_svc,\n",
    "                        X=X_train,\n",
    "                        y=y_train,\n",
    "                        cv=10,\n",
    "                        n_jobs=1)\n",
    "\n",
    "print('Cross validation scores: %s' % scores)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Cross validation scores')\n",
    "plt.scatter(np.arange(len(scores)), scores)\n",
    "plt.axhline(y=np.mean(scores), color='g') # Mean value of cross validation scores\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c73c30bb-848f-54ab-72fc-7bb8abd43ba7",
    "_uuid": "4aa4b82a81f64e1bbe5c7acf02428e63edc6f49d"
   },
   "source": [
    "# Learning Curve\n",
    "\n",
    "Learning curve enables us decide a model is over fitting to given training data and training under appropriate bias and variance balance. Now we try to plot learning curve of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3889e856-0bf1-c821-e53c-7de60e3d6b55",
    "_uuid": "2929b89de49a94054e643e364a13d136a819880f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(estimator=pipe_svc,\n",
    "                                                       X=X_train,\n",
    "                                                       y=y_train,\n",
    "                                                       train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                                                       cv=10)\n",
    "\n",
    "# Mean value of accuracy against training data\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "\n",
    "# Standard deviation of training accuracy per number of training samples\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "# Same as above for test data\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot training accuracies \n",
    "plt.plot(train_sizes, train_mean, color='red', marker='o', label='Training Accuracy')\n",
    "# Plot the variance of training accuracies\n",
    "plt.fill_between(train_sizes,\n",
    "                train_mean + train_std,\n",
    "                train_mean - train_std,\n",
    "                alpha=0.15, color='red')\n",
    "\n",
    "# Plot for test data as training data\n",
    "plt.plot(train_sizes, test_mean, color='blue', linestyle='--', marker='s', \n",
    "        label='Test Accuracy')\n",
    "plt.fill_between(train_sizes,\n",
    "                test_mean + test_std,\n",
    "                test_mean - test_std,\n",
    "                alpha=0.15, color='blue')\n",
    "\n",
    "plt.xlabel('Number of training samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e3b94f83-0f4c-782c-9dd3-a78f9f5b5eae",
    "_uuid": "be7a45d530066e7a76d5472ac78c34c6d6f6ef7a"
   },
   "source": [
    "Although the variance of score against test data is somewhat higher, we cannot see the trend of over fitting  notably because test scores also improved along with training samples.\n",
    "\n",
    "In addition, we can validation score along with a parameter. So next we try to plot validation curve.\n",
    "\n",
    "# Validation Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d1b1780c-03fc-8fad-74bd-362f6269b0d5",
    "_uuid": "b054b20a52ffd4db289f075c44bcf8eeb57be147",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "train_scores, test_scores = validation_curve(estimator=pipe_svc,\n",
    "                                             X=X_train,\n",
    "                                             y=y_train,\n",
    "                                             param_name='svc__C',\n",
    "                                             param_range=param_range,\n",
    "                                             cv=10)\n",
    "\n",
    "# Mean value of accuracy against training data\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "\n",
    "# Standard deviation of training accuracy per number of training samples\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "# Same as above for test data\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot training accuracies \n",
    "plt.plot(param_range, train_mean, color='red', marker='o', label='Training Accuracy')\n",
    "# Plot the variance of training accuracies\n",
    "plt.fill_between(param_range,\n",
    "                train_mean + train_std,\n",
    "                train_mean - train_std,\n",
    "                alpha=0.15, color='red')\n",
    "\n",
    "# Plot for test data as training data\n",
    "plt.plot(param_range, test_mean, color='blue', linestyle='--', marker='s', \n",
    "        label='Test Accuracy')\n",
    "plt.fill_between(param_range,\n",
    "                test_mean + test_std,\n",
    "                test_mean - test_std,\n",
    "                alpha=0.15, color='blue')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Regularization parameter C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bb73feec-e602-5bd8-b5db-82bcb05b40ff",
    "_uuid": "38b60d1a2ef30e4b24ffd39f1d559c2d6dfad6d6"
   },
   "source": [
    "So we can estimate the best value of C can be 1 if C is over 1 test accuracy is decreasing.\n",
    "It can cause overfitting of this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c1de0ad4-e56f-8347-4c3a-60109c9b3327",
    "_uuid": "108a88d0464a958685c51dd58d7c9d70812e44c3"
   },
   "source": [
    "So how can we decide the best hyper parameter of this model without checking learning_curve and validation curve one by one? Grid search can be an option to do this.\n",
    "\n",
    "# Grid Search\n",
    "\n",
    "Grid search enables us to search all parameter combination of give hyper parameter space and evaluate models. After grid search you can obtain the best hyper parameter set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "928d2ad1-8a45-e68c-882b-287b131b1de1",
    "_uuid": "1660c6dcefd240ae35fc7560af2a5c562bbbbd03",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "param_grid = [{'svc__C': param_range, 'svc__kernel': ['linear']},\n",
    "              {'svc__C': param_range, 'svc__gamma': param_range, \n",
    "               'svc__kernel': ['rbf']}]\n",
    "\n",
    "gs = GridSearchCV(estimator=pipe_svc,\n",
    "                  param_grid=param_grid,\n",
    "                  scoring='accuracy',\n",
    "                  cv=10)\n",
    "\n",
    "# Training and searching hyper parameter space and evaluating model\n",
    "# by using cross validation logic folded into 10\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3ae22f8a-fc9a-3ee2-dc08-6a018ab97487",
    "_uuid": "67cc7505af23df2a59832735fc33022c33f003df"
   },
   "source": [
    "Last but not least, you can check test accuracy with the best model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "723bf20b-dcfc-b79d-07e4-5658b4a0e939",
    "_uuid": "258f93df2f3a824d9ae2cbcc97a53fd89a1439fd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model = gs.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "print('Test Accuracy: %.3f' % best_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "862bdaac-deae-d9bd-3053-794f9990b9e2",
    "_uuid": "3bd5bd281bcc8b9c495ceabd8abdfd2b093be4d0"
   },
   "source": [
    "# Recap\n",
    "\n",
    "I introduced how to create hyper parameter tuned model with checking necessary metrics \n",
    "such as learning curve and validation curve.\n",
    "\n",
    "Although we cannot see any improvement by using grid search here \n",
    "(because the default parameter often provides good performance), I hope this is a good material how to use scikit learn pipeline and plot metrics. "
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
